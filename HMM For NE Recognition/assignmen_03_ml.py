# -*- coding: utf-8 -*-
"""Copy of Assignmen_03_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PDUVIMd0bsJMKpuRPiae2tbClkbN8oDv

Krishna Kant Verma

Roll No 2211CS19
"""

import pprint
import numpy as np
import pandas as pd
import nltk
import re
import warnings
warnings.filterwarnings("ignore")
import requests
import matplotlib.pyplot as plt
import seaborn as sns
import time
import random
from google.colab import files
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize

df = pd.read_csv("NER-Dataset-Train.csv")

df.head()

df.shape

df.info()

df.describe()

df.isnull().sum()

sentences_words=[]
sentences_tags=[]
file = open('NER-Dataset-Train.txt', 'r')
lines = file.readlines()
temp_sentence_words=[]
temp_sentence_tags=[]
for line in lines:
    if line=="\n":#Sentences ends at every blank line
        if len(temp_sentence_words)==0:#If zero length sentence is formed, then ignore
            continue
        sentences_words.append(temp_sentence_words)
        temp_sentence_words=[]
        sentences_tags.append(temp_sentence_tags)
        temp_sentence_tags=[]
        continue
    temp=line.split("\t")#splitting to get the tag and the word
    temp[1]=temp[1].split("\n")[0]
    temp_sentence_words.append(temp[0])
    temp_sentence_tags.append(temp[1])
file.close()

sentences_words

sum(len(row) for row in sentences_words)
sum(len(row) for row in sentences_tags)
len(sentences_words[0])

list5=[]
for i in range(len(sentences_words)):
    list4=[]
    for j in range(len(sentences_words[i])):
        list1=[]
        list1.append(sentences_words[i][j])
        list1.append(sentences_tags[i][j])
        list4.append(tuple(list1))
    list5.append(list4)

len(list5)

sum(len(row) for row in list5)

# Splitting into train and test
import random
random.seed(1)
train_set, test_set = train_test_split(list5,test_size=0.30)
print(len(train_set))
print(len(test_set))

# Getting list of tagged words
Tagged_words = [tup for sent in train_set for tup in sent]
len(Tagged_words)

# Word_Token 
Word_Token = [pair[0] for pair in Tagged_words]
print(len(Word_Token))

# vocabulary
V = set(Word_Token)
print(len(V))

# number of tags
T = set([pair[1] for pair in Tagged_words])
print(len(T))
T

"""Emission Probabilities P(w/t)"""

#Calculating P(w/t) 
t = len(T)
v = len(V)
w_given_t = np.zeros((t, v))

#Calculating  Probability of a word given a tag: Emission Probability
def prob_of_word_given_tag(word, tag, train_bag = Tagged_words):
    tag_list = [pair for pair in train_bag if pair[1]==tag]
    count_tag = len(tag_list)
    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]
    count_w_given_tag = len(w_given_tag_list)
    
    return (count_w_given_tag, count_tag)

"""Transition Probabilities P(t2/t1)"""

#Calculating the Probability of a tag given a tag: P(t2/t1) i.e. Transition Probability

def t2_given_t1(t2, t1, train_bag = Tagged_words):
    tags = [pair[1] for pair in train_bag]
    count_t1 = len([t for t in tags if t==t1])      #Counting number of occurences of t1
    count_t2_t1 = 0
    for index in range(len(tags)-1):
        if tags[index]==t1 and tags[index+1] == t2: #Counting number of times t2 follows t1
            count_t2_t1 += 1
    return (count_t2_t1, count_t1)

"""Transition matrix : Containing Probabilities of Transition From Tag1 to Tag2"""

# We will now create a Transition matrix of tags of dimension t x t
# Considering each column t2 and each row as t1
#Thus element M(i, j) is equivalent to Probability of tj given ti : P(tj given ti)

tags_matrix = np.zeros((len(T), len(T)), dtype='float32')
for i, t1 in enumerate(list(T)):
    for j, t2 in enumerate(list(T)): 
        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]

tags_matrix

# convert the matrix to a df for better readability
tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))

tags_df

tags_df.loc['O', :]

"""Visualizing the Transition Matrix on Heat Map for better intuition"""

# Heatmap of Tags matrix where T(i, j) = P(tag j given tag i)
plt.figure(figsize=(18, 12))
sns.heatmap(tags_df)
plt.show()

len(train_set)

"""Viterbi Algorithm"""

# Viterbi_Algorithm Function !
def Viterbi_Algorithm(words, train_bag = Tagged_words):
    state = []
    T = list(set([pair[1] for pair in train_bag]))
    
    for key, word in enumerate(words):
        #initializing a list of probability column for a given observation
        p = [] 
        for tag in T:
            if key == 0:
                transition_probability = tags_df.loc['O', tag]      # P(tag|start) = P(tag|'.')
            else:
                transition_probability = tags_df.loc[state[-1], tag]
                
            #Calculating emission and state probabilities
            emission_probability = prob_of_word_given_tag(words[key], tag)[0]/prob_of_word_given_tag(words[key], tag)[1]
            state_probability = emission_probability * transition_probability    
            p.append(state_probability)
            
        pmax = max(p)
        # Finding the state for which probability is maximum
        state_max = T[p.index(pmax)] 
        state.append(state_max)
    return list(zip(words, state))

"""Evaluating on Test Set

Testing

5-fold cross validation
"""

num_sents = len(list5)
k = 5
foldsize = int(num_sents/k)
foldsize

fold_accurracies = []
fold_incorrect_tags =[]
timetaken=[]
tagged_seq_collection=[]
test_seq_collection=[]
for f in range(5):
    # Locate the test set in the fold.
    test_set = list5[f*foldsize:f*foldsize+foldsize]
    # Use the rest of the sent not in test for training.
    train_set = list5[:f*foldsize] + list5[f*foldsize+foldsize:]
   

    # Getting list of tagged words
    train_tagged_words = [tup for sent in train_set for tup in sent]
    #len(train_tagged_words)

    # tokens 
    tokens = [pair[0] for pair in train_tagged_words]

    # vocabulary
    V = set(tokens)
    # print(len(V))

    # number of tags
    T = set([pair[1] for pair in train_tagged_words])
    #len(T)

    #Calculating P(w/t)
    t = len(T)
    v = len(V)
    w_given_t = np.zeros((t, v))
    
    #Calculating the Probability of a tag given a tag: P(t2/t1) i.e. Transition Probability
    def t2_given_t1(t2, t1, train_bag = train_tagged_words):
        tags = [pair[1] for pair in train_bag]
        count_t1 = len([t for t in tags if t==t1])      #Counting number of occurences of t1
        count_t2_t1 = 0
        for index in range(len(tags)-1):
            if tags[index]==t1 and tags[index+1] == t2: #Counting number of times t2 follows t1
                count_t2_t1 += 1
        return (count_t2_t1, count_t1)

    # We will now create a Transition matrix of tags of dimension t x t
    # Considering each column t2 and each row as t1
    #Thus element M(i, j) is equivalent to Probability of tj given ti : P(tj given ti)

    tags_matrix = np.zeros((len(T), len(T)), dtype='float32')
    for i, t1 in enumerate(list(T)):
        for j, t2 in enumerate(list(T)): 
            tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]

    tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))
    
    # Running the Viterbi algorithm on a few sample sentences

    random.seed(1)

    # choose random 5 sents
    #rndom = [random.randint(1,len(test_set)) for x in range(5)]

    # list of sents
    #test_run = [test_set[i] for i in rndom]

    # list of tagged words
    test_run_base = [tup for sent in test_set for tup in sent]

    # list of untagged words
    test_tagged_words = [tup[0] for sent in test_set for tup in sent]
    
    #test_run
    # tagging the test sentences
    # tagging the test sentences
    start = time.time()
    tagged_seq = Viterbi_Algorithm(test_tagged_words)
    tagged_seq_collection.append(tagged_seq)
    test_seq_collection.append(test_set)
    end = time.time()
    difference = end-start
    timetaken.append(difference)
   
    # accuracy
    check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]
    accuracy = len(check)/len(tagged_seq)
    fold_accurracies.append(accuracy)
    
    #Incorrect Tagging Tracker
    incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]
    fold_incorrect_tags.append(incorrect_tagged_cases)
   
    
    print("Fold", f)
    print('From ', f*foldsize, 'to', f*foldsize+foldsize)
    print('Accuracy =', accuracy )
    print("Time Taken :",timetaken[f])

a1=fold_accurracies[0]
a2=fold_accurracies[1]
a3=fold_accurracies[2]
a4=fold_accurracies[3]
a5=fold_accurracies[4]

labels=['Fold 0','Fold 1','Fold 2','Fold 3', 'Fold 4']
f, ax = plt.subplots(figsize=(10,5)) # set the size that you'd like (width, height)
plt.bar(labels, [a1,a2,a3,a4,a5], color=['orange','green','blue', 'red','cyan'],width=0.5)
ax.legend(fontsize = 14)

fold_incorrect_tags

"""Maximum Accuracy"""

print(max(fold_accurracies))
F=fold_accurracies.index(max(fold_accurracies))
print("Fold ",F)

"""Class wise Accuracy"""

#We will calculate Class wise of Fold with maximum accuracy 
fold_incorrect_tags[F]

#Total Incorrect Tagging in the chosen Fold is :
ic=len(fold_incorrect_tags[F])
print("Total Incorrect Tagging in the chosen Fold is :",ic)

length=len((fold_incorrect_tags[F]))
length

lista=[]
for i in range(0,length):
    lista.append(((fold_incorrect_tags[F][i])[1])[0])
len(lista)

lista

#Using lista we are extracting the tags that were incorrectly attached to some word and storing the tags in listb
listb=[]
for i in range(0,len(lista)):
    listb.append((lista[i][1]))

#We will count number of times each tag in listb was incorrectly attached to some word
from collections import Counter
Counter(listb)
dicta=dict(Counter(listb))
print(dicta)

actual_freq_of_tag=[]
for i in range(0,len(test_seq_collection[F])):
    for j in range(0,len(test_seq_collection[F][i])):
        actual_freq_of_tag.append((test_seq_collection[F][i][j][1]))

from collections import Counter
Counter(actual_freq_of_tag)
dictb=dict(Counter(actual_freq_of_tag))
print(dictb)

#of times a tag appeared in the dataset
appearance=[]
for i in range(0,len(tagged_seq_collection[F])):
        appearance.append((tagged_seq_collection[F][i][1]))
from collections import Counter
Counter(appearance)
dictappear=dict(Counter(appearance))
print(dictappear)

keyList=dictappear.keys()
keyList

#Creating a dictionary 'd' with all tags that were used through out the process and initializing it to zero
d={}
for i in keyList: 
    d[i] = 0

#With the help of dicta I will insert values in this new dict 'd'. 
for i in dicta.keys(): 
    d[i] = dicta[i]
d

#Using both dictb and d , Calculating the class wise accuracy and storing it in a new dictionary tag_ac
tag_ac={}
for i in d.keys(): 
    x = ((dictappear[i]-d[i])/(dictappear[i]))*100
    tag_ac[i]=x
tag_ac

import matplotlib.pylab as plt
f, ax = plt.subplots(figsize=(18,5)) # set the size that you'd like (width, height)
plt.bar(tag_ac.keys(), tag_ac.values(), color=['orange','green'],align='center')
ax.legend(fontsize = 14)

#tagged_seq_collection is a list of tagged_sequences of each fold
#test_seq_collection is a list of test_seq of each fold
print(len(tagged_seq_collection))
print(len(test_seq_collection))

#Extracting the Tagged Sequences and Test Sequences of the maximum fold
tseq=[]
tset=[]
for i in range(0,len(test_seq_collection[F])):
    for j in range(0,len(test_seq_collection[F][i])):
        tset.append((test_seq_collection[F][i][j][1]))

for i in range(0,len(tagged_seq_collection[F])):
        tseq.append((tagged_seq_collection[F][i][1]))

len(tseq),len(tset)

#Storing allocated tags in list ltseq
ltseq=tseq

#Storing  Actual tags of test_set in list ltset
ltset=tset
ltset

#Ultseq is list of unique tags present in the ltseq
#Ultset is list of unique tags present in the ltset
Ultset=list(set(ltset))
Ultseq=list(set(ltseq))
max(len(Ultset),len(Ultseq))

Ultset

from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt
y_true = ltset
y_pred = ltseq
array=confusion_matrix(y_true, y_pred)
array

df_cm = pd.DataFrame(array, index = [i for i in Ultset],columns = [i for i in Ultset])
plt.figure(figsize = (20,20))
sn.heatmap(df_cm, annot=True,cmap='Greens_r')

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_true, y_pred))

"""We will now feed the NER-Dataset--TestSet.csv into our model."""

dafra=pd.read_csv("NER-Dataset-TestSet.csv")

dafra.head()

dafra.shape

dafra.isnull().sum()

dafralist=[]
for i in range(0,1891):
    dafralist.append(dafra["@SammieLynnsMom"][i])

len(dafralist)

start = time.time()
Test_data_tagged_seq = Viterbi_Algorithm(dafralist)
end = time.time()
difference = end-start
print("Time Taken :",difference)

Test_data_tagged_seq
